<p align="center">

  <h1 align="center">IDEA: Inverted Text with Cooperative Deformable Aggregation for Multi-Modal Object Re-Identification</h1>

  <p align="center">
    <img src="assets/LOGO.png" alt="Description of the image" width="400" height="395">
  <p align="center">

  <p align="center">
    <a href="https://924973292.github.io/" rel="external nofollow noopener" target="_blank"><strong>Yuhao Wang</strong></a>
    Â·
    <a href="https://dblp.org/pid/51/3710-66.html" rel="external nofollow noopener" target="_blank"><strong>Yongfeng Lv</strong></a>
    Â·
    <a href="https://scholar.google.com/citations?user=MfbIbuEAAAAJ&hl=zh-CN" rel="external nofollow noopener" target="_blank"><strong>Pingping Zhang*</strong></a>
    Â·
    <a href="https://scholar.google.com/citations?user=D3nE0agAAAAJ&hl=zh-CN" rel="external nofollow noopener" target="_blank"><strong>Huchuan Lu</strong></a>
  </p>
<p align="center">
    <a href="https://arxiv.org/pdf/XXXX.XXXXX" rel="external nofollow noopener" target="_blank">CVPR 2025 Paper</a>
  <p align="center">
    <img src="assets/Motivation.png" alt="Description of the image" style="width:100%;">
  <p align="center">
<p align="center" style="font-size: 18px; color: gray;">
    Figure 1: Motivation of IDEA.
</p>
<p align="center">
    <img src="assets/Overall.png" alt="RGBNT201 assets" style="width:100%;">
</p>
<p align="center" style="font-size: 18px; color: gray;">
    Figure 2: Overall Framework of IDEA.
</p>

## **Abstract** ğŸ“
**IDEA** ğŸš€ is a novel multi-modal object Re-Identification (ReID) framework that leverages **inverted text** and **cooperative deformable aggregation** to address the challenges of complex scenarios in multi-modal imaging. By integrating semantic guidance from text annotations and adaptively aggregating discriminative local features, IDEA achieves state-of-the-art performance on multiple benchmarks.

---

## News ğŸ“¢
- We released the **IDEA** codebase!
- Great news! Our paper has been accepted to **CVPR 2025**! ğŸ†

---

## Table of Contents ğŸ“‘
- [Introduction](#introduction)
- [Contributions](#contributions)
- [Experimental Results](#experimental-results)
- [Visualizations](#visualizations)
- [Reproduction](#reproduction)
- [Citation](#citation)

---

## **Introduction** ğŸŒŸ

Multi-modal object Re-IDentification (ReID) aims to retrieve specific objects by utilizing complementary information from various modalities. However, existing methods often focus solely on fusing visual features while neglecting the potential benefits of **text-based semantic information**. 

To address this issue, we propose **IDEA**, a novel feature learning framework comprising:
1. **Inverted Multi-modal Feature Extractor (IMFE)**: Integrates multi-modal features using Modal Prefixes and an InverseNet.
2. **Cooperative Deformable Aggregation (CDA)**: Adaptively aggregates discriminative local information by generating sampling positions.

Additionally, we construct three **text-enhanced multi-modal object ReID benchmarks** using a standardized pipeline for structured and concise text annotations with Multi-modal Large Language Models (MLLMs). ğŸ“

---

## **Contributions** âœ¨

- Constructed three **text-enhanced multi-modal object ReID benchmarks**, providing a structured caption generation pipeline across multiple spectral modalities.
- Introduced **IDEA**, a novel feature learning framework with two key components:
  - **IMFE**: Integrates multi-modal features using Modal Prefixes and an InverseNet.
  - **CDA**: Adaptively aggregates discriminative local information.
- Validated the effectiveness of our approach through extensive experiments on three benchmark datasets.

---

## **Quick View** ğŸ“Š
### Dataset Examples
#### Overview of Annotations 
<p align="center">
    <img src="assets/DatasetExample.png" alt="Dataset Overview" style="width:100%;">
</p>

#### Multi-modal Person ReID Annotations Example
<p align="center">
    <img src="assets/PersonAnnoExp.png" alt="Person ReID Annotations" style="width:100%;">
</p>

#### Multi-modal Vehicle ReID Annotations Example
<p align="center">
    <img src="assets/VehicelAnnoExp.png" alt="Vehicle ReID Annotations" style="width:100%;">
</p>

### Experimental Results
#### Multi-Modal Person ReID 
<p align="center">
  <img src="assets/RGBNT201_Exp.png" alt="RGBNT201 assets" style="width:100%;">
</p>

#### Multi-Modal Vehicle ReID 
<p align="center">
    <img src="assets/RGBNT100_Exp.png" alt="RGBNT100 assets" style="width:100%;">
</p>

### Parameter Analysis
<p align="center">
    <img src="assets/ParamsCom.png" alt="Params" style="width:100%;">
</p>

---

## **Visualizations** ğŸ–¼ï¸

### Offsets Visualization
<p align="center">
    <img src="assets/Offset.png" alt="Offsets" style="width:100%;">
</p>

### Cosine Similarity Visualization
<p align="center">
    <img src="assets/CosDis.png" alt="Cosine Similarity" style="width:100%;">
</p>

### Semantic Guidance Visualization
<p align="center">
    <img src="assets/SemanticGuidance.png" alt="Semantic Guidance" style="width:100%;">
</p>

### Rank-list Visualization
#### Multi-Modal Person ReID 
<p align="center">
    <img src="assets/PersonRank.png" alt="Rank-list" style="width:100%;">
</p>

#### Multi-Modal Vehicle ReID 
<p align="center">
    <img src="assets/VehicleRank.png" alt="Rank-list" style="width:100%;">
</p>

---

## **Quick Start** ğŸš€

### Datasets
- **RGBNT201**: [Google Drive](https://drive.google.com/drive/folders/1EscBadX-wMAT56_It5lXY-S3-b5nK1wH)  
- **RGBNT100**: [Baidu Pan](https://pan.baidu.com/s/1xqqh7N4Lctm3RcUdskG0Ug) (Code: `rjin`)  
- **MSVR310**: [Google Drive](https://drive.google.com/file/d/1IxI-fGiluPO_Ies6YjDHeTEuVYhFdYwD/view?usp=drive_link)
- **Annotations**: QwenVL_Anno

### Codebase Structure
```
IDEA_Codes
â”œâ”€â”€ PTH                           # Pre-trained models
â”‚   â””â”€â”€ ViT-B-16.pt               # CLIP model
â”œâ”€â”€ DATA                          # Dataset root directory
â”‚   â”œâ”€â”€ RGBNT201                  # RGBNT201 dataset
â”‚   â”‚   â”œâ”€â”€ train_171             # Training images (171 classes)
â”‚   â”‚   â”œâ”€â”€ test                  # Testing images
â”‚   â”‚   â”œâ”€â”€ text                  # Annotations
â”‚   â”‚   â”‚   â”œâ”€â”€ train_RGB.json    # Training annotations
â”‚   â”‚   â”‚   â”œâ”€â”€ test_RGB.json     # Testing annotations
â”‚   â”‚   â”‚   â””â”€â”€ ...               # Other annotations
â”‚   â”œâ”€â”€ RGBNT100                  # RGBNT100 dataset
â”‚   â””â”€â”€ MSVR310                   # MSVR310 dataset
â”œâ”€â”€ assets                        # Github assets
â”œâ”€â”€ config                        # Configuration files
â”œâ”€â”€ QwenVL_Anno                   # **YOU SHOULD PUT YOUR ANNOTATIONS TO THE DATA FOLDER**
â””â”€â”€ ...                           # Other project files
```

### Pretrained Models
- **CLIP**: [Baidu Pan](https://pan.baidu.com/s/1YPhaL0YgpI-TQ_pSzXHRKw) (Code: `52fu`)

### Configuration
- RGBNT201: `configs/RGBNT201/IDEA.yml`  
- RGBNT100: `configs/RGBNT100/IDEA.yml`  
- MSVR310: `configs/MSVR310/IDEA.yml`

### Training
```bash
conda create -n IDEA python=3.10.13
conda activate IDEA
pip install torch==2.1.1+cu118 torchvision==0.16.1+cu118 torchaudio==2.1.1+cu118 --index-url https://download.pytorch.org/whl/cu118
cd ../IDEA_PUBLIC
pip install --upgrade pip
pip install -r requirements.txt
python train.py --config_file ./configs/RGBNT201/IDEA.yml
```

## Star History ğŸŒŸ

[![Star History Chart](https://api.star-history.com/svg?repos=924973292/IDEA&type=Date)](https://star-history.com/#924973292/IDEA&Date)

---

## **Citation** ğŸ“š

If you find **IDEA** helpful in your research, please consider citing:
```bibtex
@inproceedings{wang2025idea,
  title={IDEA: Inverted Text with Cooperative Deformable Aggregation for Multi-Modal Object Re-Identification},
  author={Wang, Yuhao and Lv, Yongfeng and Zhang, Pingping and Lu, Huchuan},
  booktitle={Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},
  year={2025}
}
```

---